% !TeX encoding = UTF-8
% !TeX root = MAIN.tex

%%%%%%%%%%%%%%%%
\chapter{Introduction}

Artificial Intelligence (AI) has undergone rapid development in the last years. In today's modern era of mobile phones and computers, algorithm's are used on a daily basis to have quick access to information and improve the efficiency of the daily life.

While various Algorithms (e.g.: Decision Trees, Linear Regression, Support Vector Machines, etc.), which are comprehensible by design, have been developed, the spotlight has turned to Deep Neural Networks(DNN). This shift is attributed to the increase in computational power and the exponential increase in accessible data. Despite their remarkable accuracy, DNN remain opaque black-boxes, which we  struggle to understand. Nevertheless, the immense improvement in performance and their ability to handle massive datasets have led to widespread adoption in contemporary devices. It is predicted, that algorithms based on Neural Networks will be becoming increasingly popular in the next years.
\\\\
However, one of the primary difficulties with Neural Networks is the lack of reliable interpretation techniques. Numerous interpretation methods exist, yet a universally reliable method remains missing. Particularly in the domain of image analysis, encompassing critical applications like automated driving and facial recognition, no solution is present. The decision-making rationale of neural networks remain unclear, attributed to factors like background elements, peripheral objects or lighting conditions.
Efforts to address this issue have given rise to gradient methods, aiming to assign significance values to pixels and represent their importance on neural network decisions.
\\
Another alternative option to mitigate the black-box nature of  algorithms involves employing model-agnostic methods. These methods offer an computational linkage between inputs and outputs, irrespective which model is used. Although highly effective for smaller datasets, they begin to struggle as the data size and their complexity increases. Because of this, they do not offer a reliable way to quickly make Neural Networks interpretable.
\\\\
In light of these prevalent problems, the object of this thesis is to recapitulate interpretation algorithms Neural networks in computer vision.  Emphasis is placed on the evaluation of post-hoc interpretability techniques, forecasting potential future developments and focusing on the strengths and weaknesses of distinct  techniques. Concluding the theoretical segment, a practical demonstration showcasing the application of ROAR is shown. 






\newpage
\section{Structure of the thesis}

\begin{enumerate}
	\item The first part presents a overview of contemporary machine learning algorithms, categorizing them into two main groups: algorithms with inherent interpretability and those without. The goal is to show why interpretation methods are required.
	\item Subsequent sections delve into interpretability, emphasizing global and local model-agnostic techniques. These methods offer insights into overall model behavior, regardless of algorithm specifics.   
	\item Additionally, in the domain of interpretability techniques for Neural Networks, the paper explores ad-hoc methods for Neural Networks. Features visualization and Gradient-focused methods are explained.
	\item After introducing existing interpretation methods, the paper's focus transitions to evaluating post-hoc interpretation methods. Various approaches to assess the effectiveness and dependability of these methods in offering meaningful insights into intricate models are introduced and discussed. Additionally, the advantages and disadvantages of these approaches are carefully examined to provide a comprehensive understanding of their applicability.
	\item To exemplify the discussed concepts, the practical application of the ROAR methodology using the food101 data set \cite{bossard14} and MNIST dataset \cite{deng2012mnist} is presented. This real-world instance illustrates effective employment of interpretability techniques in image recognition.
\end{enumerate}



%%%%%%%%%%%%%%%%
\chapter{Machine Learning and their Interpretability}


This paper focuses on two types of machine learning methods: Unsupervised and Supervised. However, we'll only look at supervised methods here because interpreting unsupervised methods works differently at a basic level.

In supervised machine learning, there are several base methods of classification. They are shortly introduced and analyzed for their interpretability. This should give readers a simply structure on why this research is needed.

\section{Supervised Methods}

This section analyzes the base functionality of each singular model. Furthermore, an analysis of the interpretability from a human perspective is made. Methods are developing rapidly, therefore only the most common methods are included.

\subsection{Linear Models}
When it comes to predicting outcomes, a simple method is to use a linear regression model. This model predicts by adding up different features, each multiplied by a weight. The linear nature of this model makes it easy to understand. Mathematically, the predictive output, denoted as $\hat{y}$, is  captured in the equation:

$$ \hat{y}= \alpha_0 + \alpha_1 x_1 + \alpha_2 x_2 +... +\alpha_n x_{n} + \epsilon$$

The alphas $\alpha_i$ indicate the significance of each feature. The initial coefficient $\alpha_0$ is known as the intercept, signifying the baseline.  The noise $\epsilon$ encapsulates the inevitable errors stemming from inherent non-linearity in real-world dynamics or measurement inaccuracies.
\\
To train model, the MSE-Loss or the absolute loss can be applied. When using regularization methods, the absolute loss is taken to be more resilient to outliers.
$$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$
$$ \text{ABS} = \frac{1}{n} \sum_{i=1}^{n} y_i - \hat{y}_i$$
\\
The interpretability of the model is very simple. The factors are given through the coefficient matrix. Each feature has a distinctive importance to the model and it can be seen easily how important each factor is, when the data is normalized. 

$$ \alpha = \begin{bmatrix}
	\alpha_0 & \\
	\alpha_1 & \\
	... & \\
	\alpha_n &
\end{bmatrix}
$$

%To customize the model and avoid over-fitting, regularization methods as Lasso-Regularization (L1) and Ridge-Regularization (L2) can be applied. The parameter lambda can be fine tuned.
%
%$$\text{Lasso Loss} = \text{Loss} + \lambda_1 \sum_{i=1}^{p} |{\alpha}_i|$$
%
%Lasso regularization is particularly useful when you want to emphasize a subset of relevant features or when dealing with multicollinearity issues. By encouraging certain coefficients to become zero, Lasso can lead to a more interpretable and sparse model.
%
%$$\text{Ridge Loss} = \text{Loss} + \lambda_2 \sum_{i=1}^{p} {\alpha}_i^2$$
%
%Ridge regularization is particularly effective when dealing with multicollinearity (highly correlated features) and helps prevent over-fitting by keeping the coefficients from taking large values. It doesn't force coefficients to be exactly zero, but it pushes them towards zero, striking a balance between fitting the data and preventing over-fitting.

Although linear models possess comprehensibility and provide a straightforward method for prediction and are inherent understandable, their application is limited to linear relationships and small datasets. In the domain of image recognition it is not applicable.

\subsection{Distance-Based methods}
K-Nearest Neighbors is used for classification and uses the nearest neighbars as classification. KNN is not interpretable by default, as there are no parameters to learn and analyze. One can argue, that KNN is interpretable by the fact, that it just describes if there are similar samples. However KNN also struggles with big datasets.

Support Vector Machines (SVM) aim to find a hyperplane that maximizes the margin between different classes of data points.

$$\text{Minimize } \frac{1}{2} \|w\|^2 + C \sum_{i=1}^{n} \xi_i$$

$$\text{Subject to } y_i (w \cdot x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0 \text{ for } i = 1, \ldots, n$$

With using the kernel trick, SVM can be used for non-linear data. In higher dimensionality, SVM becomes non-interpretable, as displaying the weight matrix is not understandable. SVM also struggles with big datasets. Therefore also SVM is not applicable for image datasets.


\subsection{Decision Tree-Based Methods}

Decision Trees based on minimizing the gini-index are inherently interpretable. As the depth increases, the models become less understandable. One disadvantage is the lack of smoothness. If one boundary is reached, the model classification changes.

Random forests are an ensemble of multiple decision trees. Their advantage is a smoother predictive power. But it suffers from being less interpretable. Using special techniques like SHAP values \cite{lundberg2017unified} or partial dependence trees can make them more understandable.

Gradient boosting like XGBoost, LightGBM and CatBoost are similar to random forest and decision trees, but with differential learned weights for each decision. They suffer from the same interpretability issues as random  forests.

While decision tree-based methods can be applied for image classification, their accuracy is poor.

\subsection{Computational intensive optimization Problems, LDA, QDA}

LDA (Linear Discriminant Analysis):
LDA aims to maximize the distance between classes while minimizing the variance within each class.
It assumes that the data within each class follows a multivariate normal distribution with equal covariance matrices across all classes.
LDA is useful when there's a linear separation between classes in the feature space.
It can be effective when the class distributions are well-separated.

QDA (Quadratic Discriminant Analysis):
QDA is an extension of LDA that relaxes the assumption of equal covariance matrices across classes.
It allows each class to have its own covariance matrix, which can capture more complex relationships between features.
QDA is suitable when the class distributions have different covariance structures or when the assumption of equal covariance is not met.

Both LDA and QDA have been applied to image recognition tasks with varying degrees of success. Their performance can be influenced by factors such as the amount of training data available, the choice of features, and the complexity of the underlying data distribution.

%Belhumeur, P. N., Hespanha, J. P., & Kriegman, D. J. (1997). Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(7), 711â€“720.
%Fukunaga, K. (1990). Introduction to Statistical Pattern Recognition (2nd ed.). Academic Press.
%Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd ed.). Springer.

\subsection{Neural Networks}

The rise of neural networks and their strong predictive power makes them a common choice for classification task. But as they increase in size, understandability by taking a look at the weights becomes impossible. With the rise of CNN in 2012  \cite{krizhevsky2012} Neural Networks have become state of art for image prediction. Special interpretability methods considering the learned weights are looked in detail in \ref{sec:nni}.


\chapter{Interpretation of Image Recognition Neural Networks}

In this chapter, evaluation method are described to analyze the behavior of not inherently interpretable models. We only introduce evaluation methods, which are used to analyze neural networks.

\section{Global Model-Agnostic Methods}

Global model-agnostic methods describe expected outcomes based on the distribution of the data. They can show a correlation between singular or multiple features and an outcome.

A quick evaluation on applicability for Neural Networks is done:

\begin{enumerate}
	\item \textbf{Partial Dependency Plots:} Partial Dependency Plots (PDP) are easy to interpret but using it with several features becomes increasingly difficult. Therefore, using a PDP is not possible in Neural Networks.
	\item \textbf{Accumulated Local Effects:} Accumulated Local Effects (ALE) are an an advancement of PDP. While it solves some problems PDP suffers from, it can't be applied to a Neural Network because of the complexity of the datasets.
	\item \textbf{Feature Interaction:} Feature Interaction also analyzes how features correlate. Doesn't work due to the same problem of the complexity of the dataset.
	\item \textbf{Functional Decomposition:} Functional Decomposition is commonly used in Neural Networks. See Chapter \ref{sec:network_dissection}.
	\item \textbf{Permutation Feature Importance:} Permutation Feature Importance is regularly used in visual machine learning tasks.
	\item \textbf{Prototype and Criticism:} Prototype and Criticism is used as adversial attacks in Neural Networks.
	
\end{enumerate}


\section{Local Model-Agnostic Methods}

To explain individual predictions, Local model-agnostic methods are used. A single outcome is correlated to some features and explain the model.


\begin{enumerate}
	\item \textbf{LIME: Local Interpretablle Model-agnostic Explanations:} Lime generates locally faithful explanations by training interpretable models on perterurbed instances of the original data.
	\item \textbf{Local surrogate models:} Local surrogate models can be programmed to select a singular instance to explain.
	\item \textbf{Scoped Rules (Anchors):} Find so called anchors to explain the predictions.
	\item \textbf{Individual conditional expectation curves:} Practically not useful in image recognition, as the computation is too high.
	\item \textbf{Counterfactual explanations:} Try to find a change while still resembling the original image.
	\item \textbf{SHAPly:} Calculate SHAP values for an image prediction to determine how each pixel contributes to the prediction's deviation from the average prediction across all images.
	Positive SHAP values indicate pixels that push the prediction up, while negative values indicate pixels that pull it down.
\end{enumerate}



\section{Neural Network Interpretation}
\label{sec:nni}
In the domain of Natural Language Processing (NLP) and Computer Vision, Deep Learning has proven very successful. By passing the input data through a sequence of layers, characterized by matrix-multiplications with kernel weights and nonlinear transformations functions, a prediction is computed. Depending on the specific task, additional elements like Long Short-Time Memory(LSTM) layers and Convolutional layers (CNN) are utilized. Given the immense amount of mathematical operations underlying a single prediction, humans are not fit to apprehend the mapping. To interpret predictions, we would have to decipher the intricate learned knowledge of numerous different kernels and weights.
Recognizing that it's impossible for humans to grasp millions of weights, the demand for evaluation methods is high. To assess the behavior and predictions of Deep Neural networks, specific interpretation methods were developed. These methods calculate the likelihood of an input entry being responsible for the result.
\\\\
While model-agnostic methods offer an approach to understand Neural Networks, the sheer size of the data used to train and test Neural Networks make this task extremely hard. For instance, an image with the dimensions of 3x224x224, as commonly encountered in Food-101,the data entries exceed 150.000. In NLP tasks, where vocabularies often encompass around 20.000 words, the computational complexity renders most model-agnostic techniques as too expensive.
\\
In the pursuit of comprehending the complex dynamics of Deep Neural Networks it makes sense to utilize the underlying weights in the model. The information saved in the hidden layers  as learned weights can be used to evaluate the network. Moreover, the gradients can be taken into consideration as well.
In the following subsections several concepts for understanding Deep Neural Networks are introduced. 


\subsection{Feature visualization and Network Dissection \cite{olah2017feature}}
\label{sec:network_dissection}
Modern Neural Networks like ResNet50 or Bard consist of several million layers. Network dissection attempts to overcome this challenge by breaking down separate layers and connecting them with ideas. 

The higher-level features in these networks relate to clear concepts, shown in Figure \ref{fig:feature-visualization}. As the input image moves through layers, it changes at each layer. In each convolutional layer, the network gains new and more complex features. The smooth joining of fully connected layers then changes image-based data into predictions.
\\
\begin{figure}[h!]
	\centering
	\includegraphics[width=150mm]{figs/FeatureVisualization}
	\caption{Feature Visualization \cite{olah2017feature}}
	\label{fig:feature-visualization}
\end{figure}
\\
The image explains this process. The first convolutional layers find simple features like edges and basic textures. Later, they recognize more detailed patterns. The deepest layers learn about parts and objects. This object information passes to the other hidden layers, which then finally make a prediction.

In the pursuit of understanding feature visualization, the focus lies on the activation of a single unit within the neural network. This involves maximizing the activation of a specific neuron, mathematically speaking (Visible in \ref*{fig:optimization}). There are two methods for achieving this. First, we can make use of the training image that triggers the highest activation. Yet, this approach faces a significant problem. When an image contains multiple objects, it's hard to pinpoint which object causes the activation. Because of this an alternative route is adopted: generating new images from random noise. This is accomplished through methods like Generative Adversarial Networks (GANs) or other diffusion-based techniques.

\begin{figure}[h!]
	\centering
	\includegraphics[width=150mm]{figs/ab}
	\caption{Activation Maximization \cite{olah2017feature}}
	\label{fig:optimization}
\end{figure}

Advantages of Feature Visualization:
\begin{enumerate}
	\item \textbf{Initial Model Insights:} Feature visualization offer an initial view into a model's behavior, improving the understanding of its inner layers.
	\item \textbf{Enhanced Domain Understanding:} It has the potential to enrich domain understanding by aligning learned features with domain-specific knowledge. An example can be seen in the medical industry
	\item \textbf{Debugging and Improvement:} Feature visualization assists in debugging and refining models, contributing to their overall performance enhancement.
\end{enumerate}

Disadvantages of Feature Visualization:
\begin{enumerate}
	\item \textbf{Unclear Decision-Making:} While activations are evident, understanding the meaning behind them and how they contribute to decision-making remains challenging.
	\item \textbf{Subjective Interpretation:} The interpretation of visualized features can be subjective, potentially leading to differing conclusions among observers.
	\item \textbf{Limited Applicability to Visual Data:} Feature visualization's applicability is  limited to visual data types.
\end{enumerate}


\subsection{Saliency Maps}

Saliency maps are visualizations that highlight the regions of an input image that have the most significant impact on a model's output. By revealing the areas that strongly influence a prediction, saliency maps bridge the gap between the model's "black-box" nature and human understanding.

\begin{figure}[h!]
	\centering
	\includegraphics[width=150mm]{figs/SaliencyExample}
	\caption{Saliency Map - Source: https://captum.ai/tutorials/Resnet\_TorchVision\_Interpret}
	\label{fig:saliency}
\end{figure}

Saliency maps are commonly calculated using SHAP \cite{lundberg2017unified} or gradient methods. Saliency maps provide a direct and intuitive way to understand which parts of an input data are influencing a model's decision. The main difficulty is the generation of reliable saliency maps.


\subsection{Gradient-focused methods}
\label{IG}

Option 1: Vanilla Gradient \& DeconvNet

Vanilla Gradient \cite{simonyan2014deep} focuses on computing the gradients of the network.
A forward pass of an image is generated. The gradients of the class score is computed. Then the gradients are visualized. When ReLU is used and the gradients are negative, then information is lost. 
Deconv Net \cite{zeiler2013visualizing} takes care of this problem.


Option 2: Grad-CAM \& Guided Grad-CAM

Grad-CAM only takes care of non-CNN maps.
Guided Grad-CAM computes Grad-CaAM with another method to have a better localization




\chapter{Evaluation of post-hoc interpretability methods}

Gradient-methods which generate saliency maps are hard to measure. While humans can evaluate the maps by giving a general statement, this is not a scientific statem<ent and can also not be applied to thousands of images. Despite many significant recent contributions to saliency maps, the valuable effort of explaning machine learning models face this methodological challenge: the difficulty of assessing the scope and quality of model explanations \cite{adebayo2020sanity}.


\section{A benchmark for interpretability methods in deep neural networks}

RoaR is a algorithm which estimates the effectiveness of saliency maps. This is done, by removing supposedly informative features from the input and observing the reaction of the neural network.

As shown in the paper, KAR seems to be a bad method to evaluate the efficiency.


\section{New Definitions and Evaluations for Saliency Methods: Staying intrinsic, complete and sound \cite{gupta2022new}}



Explaining Completeness:

Logical reasoning: All correct statements are proveable. Example: The dog is responsible for the nets output as 
dog. 

Soundness: Incorrect statements cannot be proved. Example: The sun is responsible for the nets output as dog.




\section{Sanity checks for Saliency maps}



\section{Comparison of the Evaluation Methods \& critique}

While the described methods both offer a numerical evaluation method, they do still lack a clear evaluation structure. They show, that some evaluation methods are indeed correct, but they still suffer ambuigity from different datasets.




%%%%%%%%%%%%%%%%
\chapter{Project work} % 4 pages


\section{Project Goal}
In RoaR\cite{hooker2019benchmark} the paper does not list the standard deviation of the trained nets. We expect to validate the results by achieving similar results.

As training 25 image nets requires high computational power we do not have right now, we limited our research to evaluating food-101 \cite{bossard14} using only 2 interpretation methods and comparing it to the baseline.


Additionally, we also add a mini evaluation using the MNIST dataset.



\subsection{Project Setup}




\subsection{Results and Plots}

\subsection{Discussion of results}



% finding the parameters was done by a grid search and looking at the overfitting chart.

%%%%%

