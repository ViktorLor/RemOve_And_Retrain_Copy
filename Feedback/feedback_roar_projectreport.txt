Notes on Practical Report
-------------------------
Overall: Very nice report already! Language and content looks good, in what follows I have mostly rather nit-picky comments / suggestions. 

Abstract
- 'saliency maps' could also be a specific algorithm (such as integrated gradients), so maybe you want to use something like 'attribution maps' if you look for a more general term (I think this is relevant throughout the report)
- last sentence: which problem exactly? :)

Scientific Motivation and Goal
- Several methods have been proposed for generating salience maps, including gradient-based methods, perturbation-based methods, and activation-based methods. -> could we add a reference here?
- could we elaborate on what 'effectiveness' is in this context? what is the goal we want to achieve with attribution maps precisely?
- This is because there are often multiple valid interpretations of a given input, and what might be salient to one observer may not be salient to another -> do you have a source for this? indeed it might differ for different people, but we do not really want to capture what (human) observers find salient, but rather the model we investigate
- The goal is to evaluate Integrated Gradients and find out if Remove and Retrain is giving reasonable results. -> This sounds a bit as if we'd evaluate ROAR as well, so maybe this could use some re-phrasing

Trying out various methods
- for training saliency models => applying ROAR? if you want, you can introduce this as an acronym btw
- good idea to include this section! for your thesis, we might add some (approximate) numbers here why we e.g. did not investigate ImageNet in the end, but for this report this is more than sufficient

Training, Training the Model
- can you add variance / std here? (98% +- x)

Dataset Preparation and Splitting
- is it easy to evaluate the effectiveness? why is that so? :D (maybe you could write the reasoning here instead, why you think using this dataset makes evaluation easy)
- what is the PyTorch default splitting method, could you explain this? is it random shuffling and then taking a certain percentage for training/validation?

Retraining on Saliency Maps
- I think you could actually use this section to explain ROAR (with its goal, how to achieve this, and everything) for the reader who might not know anything about ROAR - then the remaining sections, in which you discuss more about our specific choices & setup will be a lot clearer

Generating the New Datasets
- The integrated gradient method was used to assess the performance of different datasets. -> is it?
- just a small, mostly stylistic suggestion: The attribution maps were evaluated based on their ability (or: based on whether they are able) to identify the most crucial pixels, ...
- I think it would be helpful to state what you are enumerating here exactly
- The jump from replacing the highest attributions with the MNIST mean to Figure 1 is quite unexpected. Maybe you could introduce what exactly the random baseline is first? Potentially this could even be a separate sub-section, but you'll have to see. It would also be helpful to have a more thorough explanation on what can be seen in Figure 1, e.g. that the first row is attribution-based and the second random, what the columns are etc (I know that this can also be seen in the labels, but it's not very obvious and quite small - additionally, a bit of information redundancy doesn't hurt from time to time)

Retraining on the New Datasets
- what is an 'unweighted' network, one that is not trained?

Advantages and Disadvantages
- I don't completely see why the mentioned disadvantage would be of our setup, this is a general problem

Results
- I would suggest a slight restructuring of this. Referring to the plot first thing in the section is again quite a jump. Maybe you could start with talking about what we would expect to happen when removing certain percentages, both for a random baseline as well as a more advanced method? Similar to what you did in A. Are the results meaningful?. 
- Continuing with section A: this is basically a direct quote, which would require "" etc - direct quotes, in particular as long as this paragraph, should be avoided in scientific writing if possible. Maybe you could try to summarise it in your own words, and make it a 'regular' citation ([1])? 
- After talking about what we expect, you can refer to the figure / table and explain them briefly in the text. Then you can interpret the results, i.e. talk about how IG performs in contrast to the baseline, and how the standard deviations change etc (like is done in B). 
- I personally would be careful with using terms like 'proof' when it's not really a proof, and 'significant' if we didn't do a statistical significance test (unless you did one :)). 
- If you are still looking for something to add to your report, you could include a brief analysis in how your results compare to the ROAR paper, and/or what your next steps could be. 

References
- All references should be cited, I'm not sure whether this is the case
- You could try to format the references a bit more consistently. You could check out https://dblp.org/ if you don't know about that page yet - this typically gives you more complete & more consistent citations than e.g. Google Scholar does (but still not perfect ones).

Appendix
- Typically all figures / tables are being referred to in text, but for this report it's fine for me if you leave them as standalone tables like this.

